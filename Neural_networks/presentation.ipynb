{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for binary selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The problem consints in selecting accepitance of 2d arrays. The data is from the \"microchip acceptance\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will start with data and paramiters arrange, then present the functions implemented and finish with the training and cloncusions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we need to import the used libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we have to import and conditionate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_path = os.path.join(current_dir, \"classification2.txt\")\n",
    "\n",
    "try:\n",
    "    with open(data_path, \"r\") as input_file:\n",
    "        training_data = input_file.readlines()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {data_path}\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "training_data = [line.strip().split(\",\") for line in training_data]\n",
    "input_file.close()\n",
    "input_data = np.array([[float(data[0]), float(data[1])] for data in training_data], dtype=np.float32)\n",
    "X = input_data\n",
    "Y = np.array([float(data[2]) for data in training_data], dtype=np.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also (CHANGE THIS:) \"conditionate\" X values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = input_data.mean(axis=0)\n",
    "X_std = input_data.std(axis=0)\n",
    "data = (input_data - X_mean) / (X_std + 1e-8)\n",
    "X = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamiters and paramiters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(n_out, n_in):\n",
    "    return np.random.randn(n_out, n_in) * np.sqrt(1.0 / n_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(X) \n",
    "init_epsilon = 1e-2\n",
    "learning_rate = 0.0001\n",
    "epochs = 10000\n",
    "lbd = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this for the first NN\n",
    "shapes = [(10, 3), (7, 11), (1, 8)] #it's the form of the NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this for the second NN \n",
    "shapes = [(6, 3), (4, 7), (1, 5)] #it's the form of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = xavier_init(shapes[0][0],shapes[0][1])  \n",
    "theta1 = xavier_init(shapes[1][0],shapes[1][1])\n",
    "theta2 = xavier_init(shapes[2][0],shapes[2][1])  \n",
    "\n",
    "W = [theta0, theta1, theta2]\n",
    "n_layers = len(W) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unrolled tethas will be used for op.minimize implemantation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.concatenate([w.flatten() for w in W])\n",
    "initial_theta = np.hstack(theta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize theta with provided fnction\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can start with the functions. Activation used will be sigmoid, error is binary selection error (cross entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z, derivative=False):\n",
    "    sig = 1 / (1 + np.exp(-z))\n",
    "    if derivative:\n",
    "        return z * (1 - z)\n",
    "    return sig\n",
    "\n",
    "def bin_logistic_error(t,y):\n",
    "    eps = 1e-8\n",
    "    return -np.mean(y * np.log(t + eps) + (1 - y) * np.log(1 - t + eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gradient will be checked for ensure it's worth, for simplification a modified version of gradient descent was created:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, W, l, activation_func=sigmoid):\n",
    "    Z = []\n",
    "    A = [X.reshape(-1)]  # Ensure it's a flat vector\n",
    "    AWB = [np.insert(X, 0, 1)]  # Add bias term to input\n",
    "\n",
    "    for i in range(l - 1):\n",
    "        z = np.dot(W[i], AWB[i])\n",
    "        Z.append(z)\n",
    "        a = activation_func(z)\n",
    "        A.append(a)\n",
    "        AWB.append(np.insert(a, 0, 1))  # Add bias for next layer\n",
    "\n",
    "    return A, AWB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop( Y, A, AWB, W, l, activation_derivative=sigmoid):\n",
    "    gradients = []\n",
    "    deltas = []\n",
    "\n",
    "    # Output layer\n",
    "    a_output = A[-1]                  # (n_output,)\n",
    "    a_prev = AWB[-2]                  # (n_hidden + 1,)\n",
    "    error = a_output - Y              # (n_output,)\n",
    "    delta = error.reshape(-1, 1) * sigmoid(a_output, derivative=True)    # (n_output, 1)\n",
    "    grad = np.dot(delta, a_prev.reshape(1, -1))  # (n_output, n_hidden + 1)\n",
    "    deltas.insert(0, delta)\n",
    "    gradients.insert(0, grad)\n",
    "\n",
    "    # Hidden layers\n",
    "    for layer in range(l - 2, 0, -1):\n",
    "        w_next = W[layer]\n",
    "        w_next_no_bias = w_next[:, 1:]\n",
    "        delta_next = deltas[0]\n",
    "\n",
    "        a_curr = A[layer]\n",
    "        act_deriv = activation_derivative(a_curr, derivative=True).reshape(-1, 1)\n",
    "\n",
    "        delta = np.dot(w_next_no_bias.T, delta_next) * act_deriv\n",
    "        deltas.insert(0, delta)\n",
    "\n",
    "        a_prev = AWB[layer - 1]\n",
    "        grad = np.dot(delta, a_prev.reshape(1, -1))\n",
    "        gradients.insert(0, grad)\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(W, X, Y, error_func=bin_logistic_error):\n",
    "    lbd = 0  # Regularization parameter\n",
    "    m = len(Y)  # Number of training examples\n",
    "\n",
    "    shapes = [(10, 3), (7, 11), (1, 8)]\n",
    "    sizes = [np.prod(shape) for shape in shapes]\n",
    "    split_points = np.cumsum([0] + sizes)\n",
    "    reshaped_W = [\n",
    "        W[split_points[i]:split_points[i+1]].reshape(shapes[i])\n",
    "        for i in range(len(shapes))\n",
    "    ]\n",
    "\n",
    "    # Get the output layer activations for all training examples\n",
    "    A = []\n",
    "    for i in range(m):\n",
    "        activations, _ = forward_prop(X[i], reshaped_W, len(reshaped_W) + 1)\n",
    "        A.append(activations[-1])  # Output layer activation\n",
    "\n",
    "    # Calculate the cost\n",
    "    cost = error_func(np.array(A), Y) / (2 * m)\n",
    "\n",
    "    # Regularization term\n",
    "    if lbd > 0:\n",
    "        reg_sum = 0\n",
    "        for i in range(len(W)):\n",
    "            # Exclude bias terms from regularization\n",
    "            reg_sum += np.sum(W[i][:, 1:] ** 2)  # Sum of squares of non-bias weights\n",
    "        reg_term = (lbd / (2 * m)) * reg_sum\n",
    "        cost += reg_term\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient_validation(theta, X, Y, epsilon=1e-5):\n",
    "    grad = np.zeros_like(theta)\n",
    "    for i in range(len(theta)):\n",
    "        theta_eps1 = np.copy(theta)\n",
    "        theta_eps2 = np.copy(theta)\n",
    "        theta_eps1[i] += epsilon\n",
    "        theta_eps2[i] -= epsilon\n",
    "        loss1 = J(theta_eps1, X, Y)\n",
    "        loss2 = J(theta_eps2, X, Y)\n",
    "        grad[i] = (loss1 - loss2) / (2 * epsilon)\n",
    "    return grad\n",
    "\n",
    "def gradient_descent_for_grad_check(X, Y, w: list,lbd=0):\n",
    "    M = len(X)\n",
    "    n_layers = len(w) + 1\n",
    "    W = [wi.copy() for wi in w]\n",
    "\n",
    "    dW_total = [np.zeros_like(wi) for wi in W]\n",
    "    total_loss = 0\n",
    "\n",
    "    for inp in range(M):\n",
    "        A, AWB = forward_prop(X[inp], W, n_layers)\n",
    "        y_hat = float(A[-1])\n",
    "        y_true = float(Y[inp])\n",
    "        # Compute loss\n",
    "        loss = -y_true * np.log(y_hat + 1e-8) - (1 - y_true) * np.log(1 - y_hat + 1e-8)\n",
    "        total_loss += loss\n",
    "\n",
    "        dW = backward_prop(Y[inp], A, AWB, W, n_layers)\n",
    "        for i in range(len(W)):\n",
    "            dW_total[i] += dW[i]\n",
    "\n",
    "    grad_list = []\n",
    "    for i in range(len(W)):\n",
    "        grad = dW_total[i] / M\n",
    "\n",
    "        grad[:, 1:] += lbd * W[i][:, 1:]  # regularize only non-bias\n",
    "        grad_list.append(grad)\n",
    "            \n",
    "\n",
    "    return grad_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
