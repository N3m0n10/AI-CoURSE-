###############
##### Plotando fronteira de decisão não-linear
###############

from numpy import array, linspace, zeros , exp, insert, dot
import matplotlib.pyplot as plt

theta = [array([[-0.6981569 ,  1.1364996 ,  0.69351262],
       [ 0.59341225, -1.44576072, -0.81256536],
       [-0.08953905,  0.34076692, -0.16876264],
       [ 0.43608152, -1.17021788, -0.076353  ],
       [ 0.443593  ,  0.88206989, -0.7521419 ],
       [ 0.31294575, -0.54788231,  0.73708976],
       [-0.52475111,  0.49399669, -0.35315684],
       [ 0.39738004,  0.15853803, -0.09653237],
       [-0.41386226, -0.21805962, -0.0086737 ],
       [-0.36852084,  0.07712459, -0.10634045]]), array([[ 0.09276655,  0.13409629, -0.28334977, -0.11224649, -0.25917318,
         0.29978924, -0.07635991,  0.15726723,  0.23799441, -0.46338663,
        -0.25318365],
       [-0.09696222, -0.10330196,  0.20144   , -0.2608991 , -0.18342189,
         0.3315269 ,  0.19079471,  0.06028407, -0.2344817 , -0.55799824,
         0.10455659],
       [-0.36331872,  0.1381832 , -0.04641262,  0.31858493, -0.3561384 ,
         0.01599624,  0.25350038, -0.07358456,  0.14659018,  0.14195695,
        -0.16274506],
       [ 0.13940988, -0.0142097 , -0.13494187,  0.00803905,  0.1764868 ,
         0.03469465,  0.91073576, -0.65490615, -0.10688567, -0.69116631,
        -0.43024211],
       [ 0.16185139, -0.26263901, -0.4292886 , -0.01268266,  0.3859642 ,
        -0.11253009, -0.35715205,  0.25377858, -0.26437344,  0.30084295,
        -0.33144616],
       [-0.30751771,  0.05925616,  0.09905256, -0.45301675,  0.85386156,
         0.1612893 , -0.24895766,  0.04781023, -0.37990888,  0.05364465,
        -0.42665624],
       [-0.24162672, -0.14758065, -0.16075471,  0.23825156, -0.05274932,
        -0.53932905, -0.16599575,  0.11882948, -0.18161703, -0.12415499,
        -0.4376421 ]]), array([[-0.11412379,  0.02011837,  0.11144338, -0.31384524,  0.00067405,
        -0.11972317,  0.46109401,  0.22209683]])]


# Plotando fronteira de decisão
x1s = linspace(-1,1.5,50)
x2s = linspace(-1,1.5,50)
z= zeros((len(x1s),len(x2s)))

#y = h(x) = 1/(1+exp(- z))
#z = theta.T * x

def sigmoid(z, derivative=False):
    sig = 1 / (1 + exp(-z))
    if derivative:
        return z * (1 - z)
    return sig

def net_z_output(X, W, l, activation_func=sigmoid):
    Z = []
    A = [X.reshape(-1)]  # Ensure it's a flat vector
    AWB = [insert(X, 0, 1)]  # Add bias term to input

    for i in range(l - 1):
        z = dot(W[i], AWB[i])
        Z.append(z)
        a = activation_func(z)
        A.append(a)
        AWB.append(insert(a, 0, 1))  # Add bias for next layer

    return  Z[-1]


for i in range(len(x1s)):
    for j in range(len(x2s)):
        x = array([x1s[i], x2s[j]]).reshape(2,-1)
        z[i,j] = net_z_output(x,theta,4)  # (x,theta,n_layers)
plt.contour(x1s,x2s,z.T,0)
plt.xlabel("x1")
plt.ylabel("x2")
plt.show()